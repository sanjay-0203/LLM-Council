# ğŸ›ï¸ LLM Council

**Open-Source Multi-Model AI Consensus System**

LLM Council is a production-ready Python framework that orchestrates multiple local LLMs to work together, providing more reliable and well-rounded AI responses through voting, evaluation, and consensus building.

## âœ¨ Features

- **ğŸ—³ï¸Multiple Voting Methods**: Majority, weighted, ranked-choice, Borda count, approval, and Condorcet
- **ğŸ“Š Response Evaluation**: Multi-criteria scoring with aggregated feedback
- **ğŸ¤ Consensus Building**: Synthesize insights from multiple models
- ğŸ’¬ **Debate Mode**: Structured multi-round discussions
- **ğŸ¯ Confidence Calibration**: Adaptive confidence scoring based on historical accuracy
- **ğŸ’¾ Persistence**: SQLite-based session storage
- **âš¡ Streaming Support**: Real-time response streaming
- **ğŸ”Œ Multiple Backends**: Ollama (primary), vLLM, llama.cpp

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.10+**
2. **Ollama** ([installation guide](https://ollama.ai))

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull some models
ollama pull llama3.1:8b
ollama pull mistral:7b
ollama pull phi3:medium
ollama pull gemma2:9b
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm-council.git
cd llm-council

# Install dependencies
pip install -r requirements.txt

# Or install in development mode
pip install -e .
```

### Configuration

1. Copy the example environment file:
```bash
cp .env.example .env
```

2. Edit `config.yaml` to enable your installed models:
```yaml
models:
  - name: "llama3.1:8b"
    role: "general_reasoner"
    enabled: true  # Set to true
    
  - name: "mistral:7b"
    role: "analytical_thinker"
    enabled: true  # Set to true
```

### Running

```bash
# Start the CLI
python main.py

# Or with Streamlit UI (coming soon)
# streamlit run streamlit_app.py
```

## ğŸ“– Usage Examples

### Basic Question

```python
import asyncio
import yaml
from council import LLMCouncil

async def main():
    # Load config
    with open("config.yaml") as f:
        config = yaml.safe_load(f)
    
    # Initialize council
    council = LLMCouncil(config)
    await council.initialize()
    
    # Ask a question
    result = await council.ask(
        "What are the key factors to consider when designing a database schema?"
    )
    
    print(f"Answer: {result.answer}")
    print(f"Confidence: {result.confidence:.1%}")
    print(f"Participants: {result.data['num_responses']} models")
    
    await council.shutdown()

asyncio.run(main())
```

### Debate Mode

```python
# Conduct a structured debate
debate_result = await council.debate(
    topic="Should we prioritize performance or readability in code?",
    participants=["llama3.1:8b", "mistral:7b", "gemma2:9b"]
)

for round in debate_result['history']:
    print(f"\nRound {round['round']}:")
    for arg in round['arguments']:
        print(f"  {arg['model']}: {arg['content'][:200]}...")
```

### Custom Voting

```python
from council.voting import VotingMethod

result = await council.ask(
    "Explain quantum computing",
    use_voting=True,
    use_evaluation=True,
    use_consensus=True
)
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLM Council                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Model Manager              â”‚  â”‚
â”‚  â”‚  â€¢ Ollama Backend            â”‚  â”‚
â”‚  â”‚  â€¢ vLLM Backend (optional)   â”‚  â”‚
â”‚  â”‚  â€¢ LlamaCpp Backend          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Voting System              â”‚  â”‚
â”‚  â”‚  â€¢ 7 voting methods          â”‚  â”‚
â”‚  â”‚  â€¢ Confidence weighting      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Evaluator                  â”‚  â”‚
â”‚  â”‚  â€¢ Multi-criteria scoring    â”‚  â”‚
â”‚  â”‚  â€¢ Aggregated feedback       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Consensus Builder          â”‚  â”‚
â”‚  â”‚  â€¢ Response synthesis        â”‚  â”‚
â”‚  â”‚  â€¢ Debate management         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Persistence Layer          â”‚  â”‚
â”‚  â”‚  â€¢ SQLite database           â”‚  â”‚
â”‚  â”‚  â€¢ Session history           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Model Roles

The council supports specialized roles for different models:

- **General Reasoner**: Balanced, well-rounded responses
- **Analytical Thinker**: Logic-focused, evidence-based analysis
- **Creative Thinker**: Novel perspectives and innovative ideas
- **Concise Responder**: Brief, focused answers
- **Technical Expert**: Detailed technical accuracy
- **Devil's Advocate**: Challenges assumptions
- **Synthesizer**: Combines diverse perspectives
- **Fact Checker**: Verifies claims
- **Ethical Reviewer**: Considers ethical implications

## ğŸ› ï¸ Configuration

See `config.yaml` for full configuration options:

- Council behavior (voting methods, thresholds)
- Model definitions (roles, weights, parameters)
- Evaluation criteria
- Backend settings (Ollama, vLLM, llama.cpp)
- Persistence and caching
- Logging

## ğŸ“Š Voting Methods

1. **Majority**: Simple majority wins
2. **Weighted**: Member weight Ã— confidence
3. **Unanimous**: Requires full agreement
4. **Ranked**: Instant-runoff voting
5. **Borda Count**: Points by ranking
6. **Approval**: Multiple approvals
7. **Condorcet**: Pairwise comparisons

## ğŸ”§ Development

```bash
# Install development dependencies
pip install -r requirements.txt
pip install -e ".[dev]"

# Run tests
pytest

# Format code
black council/
isort council/

# Type checking
mypy council/
```

## ğŸ“ License

MIT License - see LICENSE file

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ™ Acknowledgments

- Built with [Ollama](https://ollama.ai)
- Inspired by ensemble methods in ML
- Thanks to the open-source LLM community

## ğŸ“š Citation

If you use LLM Council in your research, please cite:

```bibtex
@software{llm_council,
  title = {LLM Council: Multi-Model AI Consensus System},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/yourusername/llm-council}
}
```

---

**Made with â¤ï¸ for the open-source AI community**
